{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS289A Project F: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5  # a small number\n",
    "# Vectorized function for hashing for np efficiency\n",
    "def w(x):\n",
    "    return np.int(hash(x)) % 1000\n",
    "\n",
    "h = np.vectorize(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some scripts to merge the SW time and HW time + features CSV files\n",
    "# no need to run again !!\n",
    "\n",
    "# path_chstone= '../data/baseline_chstone.csv'\n",
    "# chstone = pd.read_csv(path_chstone, delimiter=',')\n",
    "\n",
    "# path_random = '../data/baseline_random.csv'\n",
    "# random = pd.read_csv(path_random, delimiter=',')\n",
    "# random.sort_values(by = ['program'])\n",
    "\n",
    "# random_sw=pd.read_csv('../data/sw_perf_random.csv', delimiter=',')\n",
    "# chstone_sw=pd.read_csv('../data/sw_perf_chstone.csv', delimiter=',')\n",
    "\n",
    "# merged_random = random.merge(random_sw, left_on='program', right_on='program')\n",
    "# merged_chstone = chstone.merge(chstone_sw, left_on='program', right_on='program')\n",
    "\n",
    "# merged_random.to_csv('../data/final_random.csv', index=False)\n",
    "# merged_chstone.to_csv('../data/final_chstone.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"csmith_random_programs\"\n",
    "data = pd.read_csv('../data/final_random.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a). Pre-process the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9011, 116)\n",
      "(3862, 116)\n",
      "(12873, 116)\n",
      "[2915.27039334195 3064.5126890095776 3127.9902745113 ...\n",
      " 2596.5991783791956 2804.3891404735623 3077.551395099053]\n"
     ]
    }
   ],
   "source": [
    "# set out training set to be 70% of total; 30% \n",
    "# random_idx = random.randint(0, np.shape(data)[0]) #27\n",
    "num_train = round(np.shape(data)[0]*0.7)\n",
    "train_data = data.values[0:num_train,:]\n",
    "test_data =  data.values[num_train:, :]\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(test_data))\n",
    "print(np.shape(data))\n",
    "\n",
    "train_speedup = (train_data[:, -1] / train_data[:, 2]) # \n",
    "test_speedup = (test_data[:, -1] / test_data[:, 2]) # -O3\n",
    "print(train_speedup)\n",
    "# log_train_speedup = np.log10(train_speedup.astype(float))\n",
    "# log_test_speedup = np.log10(test_speedup.astype(float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA of speed up from training set\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.style.use('ggplot')\n",
    "plt.hist(train_speedup, bins=100, range =(0, 7000))\n",
    "plt.hist(test_speedup, bins=100, range = (0, 7000))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5433359227610698\n",
      "0.5468669083376488\n"
     ]
    }
   ],
   "source": [
    "# Get binary preduction output: is speedup (HW vs SW) > 2800 \n",
    "y = (train_speedup > 2800).astype(int)\n",
    "X = train_data[:,-57:-1]\n",
    "y_test = (test_speedup > 2800).astype(int)\n",
    "X_test = test_data[:,-57:-1]\n",
    "print(np.count_nonzero(y) / len(y))\n",
    "print(np.count_nonzero(y_test) / len(y_test))\n",
    "assert(len(y) == np.shape(train_data)[0])\n",
    "assert(len(y_test) == np.shape(test_data)[0] )\n",
    "\n",
    "features = data.columns.values[-57:-1]\n",
    "assert len(features) == 56\n",
    "class_names = [\"On-Chip\", \"Not On-Chip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, print_splits=True):\n",
    "    print(\"Cross validation\", cross_val_score(clf, X, y, cv=3))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [\n",
    "            (features[term[0]], term[1]) for term in counter.most_common()\n",
    "        ]\n",
    "        if print_splits == True:\n",
    "            print(\"First splits\", first_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "sklearn's decision tree\n",
      "Cross validation [0.76464714 0.76198402 0.78354978]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "# Basic decision tree\n",
    "print('==============================================')\n",
    "print(\"sklearn's decision tree\")\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 20,\n",
    "}\n",
    "clf = DecisionTreeClassifier(random_state=0, **params)\n",
    "clf.fit(X, y)\n",
    "evaluate(clf, print_splits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydot import graph_from_dot_data\n",
    "import io\n",
    "out = io.StringIO()\n",
    "export_graphviz(clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "# For OSX, may need the following for dot: brew install gprof2dot\n",
    "graph = graph_from_dot_data(out.getvalue())\n",
    "graph_from_dot_data(out.getvalue())[0].write_pdf(\"{}-tree-depth{}.pdf\".format(dataset, max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(random_state=i, **self.params) for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n):\n",
    "            idx = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "            newX, newy = X[idx, :], y[idx]\n",
    "            self.decision_trees[i].fit(newX, newy)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        yhat = [self.decision_trees[i].predict(X) for i in range(self.n)]\n",
    "        # TODO: compute yhat_avg for BaggedTrees\n",
    "        ### start code ###\n",
    "        yhat_avg = np.round(np.mean(yhat, axis=0), decimals=0)\n",
    "        ### end code ###\n",
    "        return yhat_avg\n",
    "# Bagged trees\n",
    "# print(\" bagged trees\")\n",
    "# bt = BaggedTrees(params, n=N)\n",
    "# bt.fit(X, y)\n",
    "# evaluate(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(BaggedTrees):\n",
    "\n",
    "    def __init__(self, params=None, n=200, m=1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params['max_features'] = m\n",
    "        super().__init__(params=params, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostedRandomForest(RandomForest):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data\n",
    "        self.a = np.zeros(self.n)  # Weights on decision trees\n",
    "        i = 0\n",
    "        while i < self.n:\n",
    "            idx = np.random.choice(X.shape[0], size=X.shape[0], p=self.w)\n",
    "            newX, newy = X[idx, :], y[idx]\n",
    "            self.decision_trees[i].fit(newX, newy)\n",
    "            wrong = np.abs((y - self.decision_trees[i].predict(X)))\n",
    "            error = wrong.dot(self.w) / np.sum(self.w)\n",
    "            self.a[i] = 0.5 * np.log((1 - error) / error)\n",
    "            # Update w\n",
    "            wrong_idx = np.where(wrong > 0.5)[0]\n",
    "            right_idx = np.where(wrong <= 0.5)[0]\n",
    "            # TODO: fill in the code for updating 'self.w'\n",
    "            \n",
    "            ### start code ###\n",
    "            self.w[wrong_idx] = self.w[wrong_idx]*np.exp(self.a[i])\n",
    "            self.w[right_idx] = self.w[right_idx]*np.exp(-self.a[i])\n",
    "            self.w = self.w / sum(self.w)\n",
    "            #print(self.w)\n",
    "            i += 1\n",
    "        ### end code ###\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        yhat = [self.a[i]*self.decision_trees[i].predict(X) for i in range(self.n)]\n",
    "        yhat_BoostedRandomForest = np.round(np.sum(yhat, axis=0) / np.sum(self.a))\n",
    "        return yhat_BoostedRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77097204 0.7563249  0.78421578]\n",
      "random forest test accuracy:  0.771620921802175\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "params_rf = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 30,\n",
    "    \"n_estimators\": 500,\n",
    "    \"min_samples_split\":2\n",
    "}\n",
    "max_depth = 5\n",
    "rf = RandomForestClassifier(**params_rf, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "rf.fit(X, y)\n",
    "print(cross_val_score(rf, X, y, scoring='accuracy', cv=3))\n",
    "print('random forest test accuracy: ', accuracy_score(rf.predict(X_test), y_test))\n",
    "print(len(rf.estimators_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.7589332  0.76359399 0.76307613 0.76488866 0.76540653\n",
      "  0.77136199 0.77084412 0.76929052 0.76825479]\n",
      " [0.         0.76229933 0.76359399 0.76721906 0.76462973 0.76644226\n",
      "  0.76954946 0.76747799 0.77032626 0.77006732]\n",
      " [0.         0.76359399 0.76644226 0.76592439 0.76411186 0.76618332\n",
      "  0.76773692 0.76773692 0.76929052 0.77162092]\n",
      " [0.         0.76566546 0.76462973 0.76566546 0.76566546 0.76721906\n",
      "  0.76773692 0.77006732 0.76773692 0.77032626]\n",
      " [0.         0.76359399 0.76437079 0.76644226 0.76566546 0.76929052\n",
      "  0.76903159 0.76644226 0.77213879 0.77110306]\n",
      " [0.         0.76514759 0.76307613 0.76437079 0.76566546 0.76540653\n",
      "  0.76773692 0.76851372 0.76980839 0.77136199]\n",
      " [0.         0.76462973 0.76462973 0.76670119 0.76644226 0.76799586\n",
      "  0.76799586 0.76877266 0.76954946 0.77136199]\n",
      " [0.         0.76385293 0.76488866 0.76437079 0.76592439 0.76644226\n",
      "  0.76825479 0.77084412 0.77006732 0.77187985]\n",
      " [0.         0.76462973 0.76307613 0.76592439 0.76721906 0.76670119\n",
      "  0.76929052 0.76954946 0.77058519 0.77032626]\n",
      " [0.         0.76359399 0.76514759 0.76592439 0.76514759 0.76566546\n",
      "  0.76877266 0.77032626 0.77110306 0.77110306]\n",
      " [0.         0.76359399 0.76229933 0.76488866 0.76566546 0.76773692\n",
      "  0.76903159 0.76980839 0.77136199 0.77084412]\n",
      " [0.         0.76385293 0.76411186 0.76385293 0.76566546 0.76618332\n",
      "  0.76954946 0.77084412 0.77084412 0.77136199]\n",
      " [0.         0.76359399 0.76540653 0.76540653 0.76333506 0.76618332\n",
      "  0.76851372 0.77058519 0.76954946 0.77032626]\n",
      " [0.         0.76204039 0.76281719 0.76592439 0.76540653 0.76514759\n",
      "  0.76851372 0.77006732 0.77136199 0.77162092]\n",
      " [0.         0.76411186 0.76333506 0.76540653 0.76618332 0.76670119\n",
      "  0.76929052 0.76980839 0.76954946 0.77084412]\n",
      " [0.         0.76385293 0.76359399 0.76514759 0.76411186 0.76799586\n",
      "  0.76799586 0.76980839 0.77136199 0.76980839]\n",
      " [0.         0.76229933 0.76488866 0.76488866 0.76488866 0.76566546\n",
      "  0.76799586 0.76954946 0.76954946 0.76980839]\n",
      " [0.         0.76255826 0.76307613 0.76514759 0.76411186 0.76618332\n",
      "  0.76903159 0.76903159 0.77032626 0.77162092]\n",
      " [0.         0.76126359 0.76540653 0.76437079 0.76462973 0.76644226\n",
      "  0.76929052 0.76877266 0.77032626 0.77162092]\n",
      " [0.         0.76255826 0.76385293 0.76411186 0.76566546 0.76618332\n",
      "  0.76929052 0.77032626 0.77187985 0.77110306]\n",
      " [0.         0.76359399 0.76488866 0.76411186 0.76488866 0.76618332\n",
      "  0.76877266 0.77110306 0.77162092 0.77110306]\n",
      " [0.         0.76462973 0.76255826 0.76514759 0.76592439 0.76540653\n",
      "  0.76825479 0.76929052 0.77058519 0.77110306]\n",
      " [0.         0.76359399 0.76462973 0.76462973 0.76618332 0.76618332\n",
      "  0.76877266 0.77006732 0.77136199 0.77136199]\n",
      " [0.         0.76385293 0.76359399 0.76437079 0.76592439 0.76644226\n",
      "  0.76825479 0.76929052 0.77084412 0.77187985]\n",
      " [0.         0.76359399 0.76437079 0.76592439 0.76514759 0.76540653\n",
      "  0.76877266 0.76929052 0.77032626 0.77110306]\n",
      " [0.         0.76333506 0.76411186 0.76540653 0.76592439 0.76592439\n",
      "  0.76903159 0.76929052 0.77136199 0.77058519]\n",
      " [0.         0.76462973 0.76333506 0.76462973 0.76462973 0.76540653\n",
      "  0.76877266 0.76980839 0.77162092 0.77213879]\n",
      " [0.         0.76437079 0.76359399 0.76566546 0.76540653 0.76618332\n",
      "  0.76877266 0.77032626 0.77239772 0.77162092]\n",
      " [0.         0.76359399 0.76411186 0.76462973 0.76514759 0.76670119\n",
      "  0.76903159 0.76954946 0.77213879 0.77136199]\n",
      " [0.         0.76462973 0.76462973 0.76411186 0.76411186 0.76592439\n",
      "  0.76799586 0.76929052 0.77162092 0.77162092]\n",
      " [0.         0.76255826 0.76437079 0.76333506 0.76540653 0.76747799\n",
      "  0.76721906 0.76929052 0.77058519 0.77136199]\n",
      " [0.         0.76307613 0.76437079 0.76618332 0.76488866 0.76618332\n",
      "  0.76825479 0.76954946 0.77006732 0.77136199]\n",
      " [0.         0.76462973 0.76514759 0.76592439 0.76514759 0.76618332\n",
      "  0.76877266 0.76980839 0.77006732 0.77162092]\n",
      " [0.         0.76333506 0.76411186 0.76437079 0.76437079 0.76644226\n",
      "  0.76825479 0.77006732 0.77162092 0.77110306]\n",
      " [0.         0.76514759 0.76462973 0.76488866 0.76540653 0.76540653\n",
      "  0.76877266 0.76903159 0.76929052 0.77110306]\n",
      " [0.         0.76411186 0.76437079 0.76566546 0.76437079 0.76644226\n",
      "  0.76903159 0.76980839 0.77136199 0.77110306]\n",
      " [0.         0.76488866 0.76333506 0.76618332 0.76488866 0.76618332\n",
      "  0.76851372 0.76980839 0.77110306 0.77136199]\n",
      " [0.         0.76385293 0.76411186 0.76514759 0.76514759 0.76618332\n",
      "  0.76825479 0.77032626 0.77084412 0.77136199]\n",
      " [0.         0.76437079 0.76385293 0.76488866 0.76566546 0.76618332\n",
      "  0.76903159 0.76980839 0.77032626 0.77110306]]\n",
      "0.7723977213878819\n",
      "(28, 8)\n"
     ]
    }
   ],
   "source": [
    "# scores=np.zeros((40,10))\n",
    "# for n in range (1, 40):\n",
    "#     for m in range(1, 10) :\n",
    "#         rf = RandomForestClassifier(n_estimators=n*20, min_samples_leaf=m*4,max_features=np.int(np.sqrt(X.shape[1])))\n",
    "#         rf.fit(X, y)\n",
    "#         scores[n,m] =  accuracy_score(rf.predict(X_test), y_test)\n",
    "print(scores)\n",
    "print(np.max(scores))\n",
    "print(np.unravel_index(np.argmax(scores), (40,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.771620921802175\n",
      "(3, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "grad_scores=np.zeros((40,10,5))\n",
    "for n in range (1, 40):\n",
    "    for m in range(1, 10) :\n",
    "        for l in range(1,5):\n",
    "            grad = GradientBoostingClassifier(n_estimators=n*20, min_samples_leaf=m*4, learning_rate=0.1*l,max_features=np.int(np.sqrt(X.shape[1])))\n",
    "            grad.fit(X, y)\n",
    "            grad_scores[n,m,l] =  accuracy_score(grad.predict(X_test), y_test)\n",
    "print(np.max(grad_scores))\n",
    "print(np.unravel_index(np.argmax(grad_scores), (40,10,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation: 0.7645 (+/- 0.01) [Logistic Regression]\n",
      "Test accuracy: 0.7589 (+/- 0.00) [Logistic Regression]\n",
      "Cross validation: 0.7614 (+/- 0.01) [naive Bayes]\n",
      "Test accuracy: 0.7576 (+/- 0.00) [naive Bayes]\n",
      "Cross validation: 0.7716 (+/- 0.01) [Random Forest]\n",
      "Test accuracy: 0.7698 (+/- 0.00) [Random Forest]\n",
      "Cross validation: 0.7646 (+/- 0.01) [Extra Trees]\n",
      "Test accuracy: 0.7582 (+/- 0.00) [Extra Trees]\n",
      "Cross validation: 0.7691 (+/- 0.01) [AdaBoost]\n",
      "Test accuracy: 0.7628 (+/- 0.00) [AdaBoost]\n",
      "Cross validation: 0.7692 (+/- 0.01) [Gradient Boosting]\n",
      "Test accuracy: 0.7644 (+/- 0.00) [Gradient Boosting]\n",
      "Cross validation: 0.7677 (+/- 0.01) [Voting Classfier]\n",
      "Test accuracy: 0.7701 (+/- 0.00) [Voting Classfier]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 10,\n",
    "    \"n_estimators\": 200,\n",
    "    \"random_state\":0,\n",
    "    \"min_samples_split\":2\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    #\"max_depth\": 5,\n",
    "    #\"min_samples_leaf\": 10,\n",
    "    \"n_estimators\": 200,\n",
    "    \"random_state\":0,\n",
    "    #\"min_samples_split\":2\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "ext = ExtraTreesClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "ada = AdaBoostClassifier(**params2)\n",
    "grad = GradientBoostingClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "eclf = VotingClassifier(estimators=[('lr', logreg), ('gnb', gnb), ('rf', rf), ('ext', ext),('ada', ada), ('grad', grad)], voting='hard')\n",
    "\n",
    "for clf, label in zip([logreg, gnb, rf, ext,ada,grad,eclf], [\n",
    "    'Logistic Regression', 'naive Bayes',\n",
    "    'Random Forest', 'Extra Trees', \"AdaBoost\", \"Gradient Boosting\",'Voting Classfier']):\n",
    "    clf.fit(X,y)\n",
    "    cross_scores = cross_val_score(clf, X, y, scoring='accuracy', cv=3)\n",
    "    test_scores = accuracy_score(clf.predict(X_test), y_test)\n",
    "    print(\"Cross validation: %0.4f (+/- %0.2f) [%s]\" % (cross_scores.mean(), cross_scores.std(), label))\n",
    "    print(\"Test accuracy: %0.4f (+/- %0.2f) [%s]\" % (test_scores.mean(), test_scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    #\"max_depth\": 5,\n",
    "    #\"min_samples_leaf\": 10,\n",
    "    \"random_state\":0,\n",
    "    \"min_samples_split\":2\n",
    "}\n",
    "\n",
    "# Constant classifier\n",
    "print('==============================================')\n",
    "print(\"Constant classifier\")\n",
    "print('constant classifier test accuracy: ', accuracy_score(np.zeros_like(y_test), y_test))\n",
    "\n",
    "\n",
    "# Basic decision tree\n",
    "print('==============================================')\n",
    "print(\"Decsion Tree classifier\")\n",
    "clf = DecisionTreeClassifier(**params)\n",
    "clf.fit(X, y)\n",
    "print(\"cross validation {}\".format(cross_val_score(rf, X, y, cv=5)))\n",
    "print('sklearn decision tree test accuracy: ', accuracy_score(clf.predict(X_test), y_test))\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 10,\n",
    "    \"n_estimators\": 300,\n",
    "    \"random_state\":0,\n",
    "    \"min_samples_split\":2\n",
    "}\n",
    "\n",
    "# Random forest\n",
    "print('==============================================')\n",
    "print(\"Random Forest classifier\")\n",
    "rf = RandomForestClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "rf.fit(X, y)\n",
    "print(\"cross validation {}\".format(cross_val_score(rf, X, y, cv=5)))\n",
    "print('random forest test accuracy: ', accuracy_score(rf.predict(X_test), y_test))\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "print('==============================================')\n",
    "print(\"Extremely Randomized Forest classifier\")\n",
    "ext = ExtraTreesClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "ext.fit(X,y)\n",
    "print(\"cross validation {}\".format(cross_val_score(rf, X, y, cv=5)))\n",
    "print('Extreme random forest test accuracy: ', accuracy_score(ext.predict(X_test), y_test))   \n",
    "                           \n",
    "# Gradient boosted random forest\n",
    "print('==============================================')\n",
    "print(\"Gradient Boosting classifier\")\n",
    "boosted = GradientBoostingClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "boosted.fit(X, y)\n",
    "print(\"cross validation {}\".format(cross_val_score(rf, X, y, cv=5)))\n",
    "print('boosted random forest test accuracy: ', accuracy_score(boosted.predict(X_test), y_test))\n",
    "# np.savetxt('{}.out'.format(dataset), boosted.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
