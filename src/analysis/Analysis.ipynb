{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS289A Project F: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5  # a small number\n",
    "# Vectorized function for hashing for np efficiency\n",
    "def w(x):\n",
    "    return np.int(hash(x)) % 1000\n",
    "\n",
    "h = np.vectorize(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some scripts to merge the SW time and HW time + features CSV files\n",
    "# no need to run again !!\n",
    "\n",
    "# path_chstone= '../data/baseline_chstone.csv'\n",
    "# chstone = pd.read_csv(path_chstone, delimiter=',')\n",
    "\n",
    "# path_random = '../data/baseline_random.csv'\n",
    "# random = pd.read_csv(path_random, delimiter=',')\n",
    "# random.sort_values(by = ['program'])\n",
    "\n",
    "# random_sw=pd.read_csv('../data/sw_perf_random.csv', delimiter=',')\n",
    "# chstone_sw=pd.read_csv('../data/sw_perf_chstone.csv', delimiter=',')\n",
    "\n",
    "# merged_random = random.merge(random_sw, left_on='program', right_on='program')\n",
    "# merged_chstone = chstone.merge(chstone_sw, left_on='program', right_on='program')\n",
    "\n",
    "# merged_random.to_csv('../data/final_random.csv', index=False)\n",
    "# merged_chstone.to_csv('../data/final_chstone.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"csmith_random_programs\"\n",
    "data = pd.read_csv('../data/final_random.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a). Pre-process the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9011, 116)\n",
      "(3862, 116)\n",
      "(12873, 116)\n",
      "[2915.27039334195 3064.5126890095776 3127.9902745113 ...\n",
      " 2596.5991783791956 2804.3891404735623 3077.551395099053]\n"
     ]
    }
   ],
   "source": [
    "# set out training set to be 70% of total; 30% \n",
    "# random_idx = random.randint(0, np.shape(data)[0]) #27\n",
    "num_train = round(np.shape(data)[0]*0.7)\n",
    "train_data = data.values[0:num_train,:]\n",
    "test_data =  data.values[num_train:, :]\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(test_data))\n",
    "print(np.shape(data))\n",
    "\n",
    "train_speedup = (train_data[:, -1] / train_data[:, 2]) # \n",
    "test_speedup = (test_data[:, -1] / test_data[:, 2]) # -O3\n",
    "print(train_speedup)\n",
    "# log_train_speedup = np.log10(train_speedup.astype(float))\n",
    "# log_test_speedup = np.log10(test_speedup.astype(float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA of speed up from training set\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.style.use('ggplot')\n",
    "plt.hist(train_speedup, bins=100, range =(0, 7000))\n",
    "plt.hist(test_speedup, bins=100, range = (0, 7000))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5433359227610698\n",
      "0.5468669083376488\n"
     ]
    }
   ],
   "source": [
    "# Get binary preduction output: is speedup (HW vs SW) > 2800 \n",
    "y = (train_speedup > 2800).astype(int)\n",
    "X = train_data[:,-57:-1]\n",
    "y_test = (test_speedup > 2800).astype(int)\n",
    "X_test = test_data[:,-57:-1]\n",
    "print(np.count_nonzero(y) / len(y))\n",
    "print(np.count_nonzero(y_test) / len(y_test))\n",
    "assert(len(y) == np.shape(train_data)[0])\n",
    "assert(len(y_test) == np.shape(test_data)[0] )\n",
    "\n",
    "features = data.columns.values[-57:-1]\n",
    "assert len(features) == 56\n",
    "class_names = [\"On-Chip\", \"Not On-Chip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, print_splits=True):\n",
    "    print(\"Cross validation\", cross_val_score(clf, X, y, cv=3))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [\n",
    "            (features[term[0]], term[1]) for term in counter.most_common()\n",
    "        ]\n",
    "        if print_splits == True:\n",
    "            print(\"First splits\", first_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "sklearn's decision tree\n",
      "Cross validation [0.76464714 0.76198402 0.78354978]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "# Basic decision tree\n",
    "print('==============================================')\n",
    "print(\"sklearn's decision tree\")\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 20,\n",
    "}\n",
    "clf = DecisionTreeClassifier(random_state=0, **params)\n",
    "clf.fit(X, y)\n",
    "evaluate(clf, print_splits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydot import graph_from_dot_data\n",
    "import io\n",
    "out = io.StringIO()\n",
    "export_graphviz(clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "# For OSX, may need the following for dot: brew install gprof2dot\n",
    "graph = graph_from_dot_data(out.getvalue())\n",
    "graph_from_dot_data(out.getvalue())[0].write_pdf(\"{}-tree-depth{}.pdf\".format(dataset, max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(random_state=i, **self.params) for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n):\n",
    "            idx = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "            newX, newy = X[idx, :], y[idx]\n",
    "            self.decision_trees[i].fit(newX, newy)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        yhat = [self.decision_trees[i].predict(X) for i in range(self.n)]\n",
    "        # TODO: compute yhat_avg for BaggedTrees\n",
    "        ### start code ###\n",
    "        yhat_avg = np.round(np.mean(yhat, axis=0), decimals=0)\n",
    "        ### end code ###\n",
    "        return yhat_avg\n",
    "# Bagged trees\n",
    "# print(\" bagged trees\")\n",
    "# bt = BaggedTrees(params, n=N)\n",
    "# bt.fit(X, y)\n",
    "# evaluate(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(BaggedTrees):\n",
    "\n",
    "    def __init__(self, params=None, n=200, m=1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params['max_features'] = m\n",
    "        super().__init__(params=params, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostedRandomForest(RandomForest):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data\n",
    "        self.a = np.zeros(self.n)  # Weights on decision trees\n",
    "        i = 0\n",
    "        while i < self.n:\n",
    "            idx = np.random.choice(X.shape[0], size=X.shape[0], p=self.w)\n",
    "            newX, newy = X[idx, :], y[idx]\n",
    "            self.decision_trees[i].fit(newX, newy)\n",
    "            wrong = np.abs((y - self.decision_trees[i].predict(X)))\n",
    "            error = wrong.dot(self.w) / np.sum(self.w)\n",
    "            self.a[i] = 0.5 * np.log((1 - error) / error)\n",
    "            # Update w\n",
    "            wrong_idx = np.where(wrong > 0.5)[0]\n",
    "            right_idx = np.where(wrong <= 0.5)[0]\n",
    "            # TODO: fill in the code for updating 'self.w'\n",
    "            \n",
    "            ### start code ###\n",
    "            self.w[wrong_idx] = self.w[wrong_idx]*np.exp(self.a[i])\n",
    "            self.w[right_idx] = self.w[right_idx]*np.exp(-self.a[i])\n",
    "            self.w = self.w / sum(self.w)\n",
    "            #print(self.w)\n",
    "            i += 1\n",
    "        ### end code ###\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        yhat = [self.a[i]*self.decision_trees[i].predict(X) for i in range(self.n)]\n",
    "        yhat_BoostedRandomForest = np.round(np.sum(yhat, axis=0) / np.sum(self.a))\n",
    "        return yhat_BoostedRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation [0.77463382 0.75865513 0.78121878]\n",
      "random forest test accuracy:  0.7698083894355257\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 10,\n",
    "    \"n_estimators\": 200\n",
    "}\n",
    "N = 200\n",
    "max_depth = 5\n",
    "#rf = RandomForestClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "rf = RandomForestClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "rf.fit(X, y)\n",
    "evaluate(rf, print_splits=True)\n",
    "print('random forest test accuracy: ', accuracy_score(rf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Constant classifier\n",
      "constant classifier test accuracy:  0.4531330916623511\n",
      "==============================================\n",
      "Decsion Tree classifier\n",
      "cross validation [0.77703827 0.77136515 0.76415094 0.77136515 0.78079911]\n",
      "sklearn decision tree test accuracy:  0.6802175038839979\n",
      "==============================================\n",
      "Random Forest classifier\n",
      "cross validation [0.7775929  0.77247503 0.76470588 0.77413984 0.78135405]\n",
      "random forest test accuracy:  0.7700673226307613\n",
      "==============================================\n",
      "Extremely Randomized Forest classifier\n",
      "cross validation [0.7775929  0.77247503 0.76470588 0.77413984 0.78135405]\n",
      "Extreme random forest test accuracy:  0.7589331952356292\n",
      "==============================================\n",
      "Gradient Boosting classifier\n",
      "cross validation [0.7775929  0.77247503 0.76470588 0.77413984 0.78135405]\n",
      "boosted random forest test accuracy:  0.7599689280165717\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    #\"max_depth\": 5,\n",
    "    #\"min_samples_leaf\": 10,\n",
    "    \"random_state\":0,\n",
    "    \"min_samples_split\":2\n",
    "}\n",
    "\n",
    "# Constant classifier\n",
    "print('==============================================')\n",
    "print(\"Constant classifier\")\n",
    "print('constant classifier test accuracy: ', accuracy_score(np.zeros_like(y_test), y_test))\n",
    "\n",
    "\n",
    "# Basic decision tree\n",
    "print('==============================================')\n",
    "print(\"Decsion Tree classifier\")\n",
    "clf = DecisionTreeClassifier(**params)\n",
    "clf.fit(X, y)\n",
    "print(\"cross validation {}\".format(cross_val_score(rf, X, y, cv=5)))\n",
    "print('sklearn decision tree test accuracy: ', accuracy_score(clf.predict(X_test), y_test))\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 10,\n",
    "    \"n_estimators\": 300,\n",
    "    \"random_state\":0,\n",
    "    \"min_samples_split\":2\n",
    "}\n",
    "\n",
    "# Random forest\n",
    "print('==============================================')\n",
    "print(\"Random Forest classifier\")\n",
    "rf = RandomForestClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "rf.fit(X, y)\n",
    "print(\"cross validation {}\".format(cross_val_score(rf, X, y, cv=5)))\n",
    "print('random forest test accuracy: ', accuracy_score(rf.predict(X_test), y_test))\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "print('==============================================')\n",
    "print(\"Extremely Randomized Forest classifier\")\n",
    "ext = ExtraTreesClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "ext.fit(X,y)\n",
    "print(\"cross validation {}\".format(cross_val_score(rf, X, y, cv=5)))\n",
    "print('Extreme random forest test accuracy: ', accuracy_score(ext.predict(X_test), y_test))   \n",
    "                           \n",
    "# Gradient boosted random forest\n",
    "print('==============================================')\n",
    "print(\"Gradient Boosting classifier\")\n",
    "boosted = GradientBoostingClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "boosted.fit(X, y)\n",
    "print(\"cross validation {}\".format(cross_val_score(rf, X, y, cv=5)))\n",
    "print('boosted random forest test accuracy: ', accuracy_score(boosted.predict(X_test), y_test))\n",
    "# np.savetxt('{}.out'.format(dataset), boosted.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation: 0.7645 (+/- 0.01) [Logistic Regression]\n",
      "Test accuracy: 0.7589 (+/- 0.00) [Logistic Regression]\n",
      "Cross validation: 0.7614 (+/- 0.01) [naive Bayes]\n",
      "Test accuracy: 0.7576 (+/- 0.00) [naive Bayes]\n",
      "Cross validation: 0.7716 (+/- 0.01) [Random Forest]\n",
      "Test accuracy: 0.7698 (+/- 0.00) [Random Forest]\n",
      "Cross validation: 0.7646 (+/- 0.01) [Extra Trees]\n",
      "Test accuracy: 0.7582 (+/- 0.00) [Extra Trees]\n",
      "Cross validation: 0.7691 (+/- 0.01) [AdaBoost]\n",
      "Test accuracy: 0.7628 (+/- 0.00) [AdaBoost]\n",
      "Cross validation: 0.7692 (+/- 0.01) [Gradient Boosting]\n",
      "Test accuracy: 0.7644 (+/- 0.00) [Gradient Boosting]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 10,\n",
    "    \"n_estimators\": 200,\n",
    "    \"random_state\":0,\n",
    "    \"min_samples_split\":2\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    #\"max_depth\": 5,\n",
    "    #\"min_samples_leaf\": 10,\n",
    "    \"n_estimators\": 200,\n",
    "    \"random_state\":0,\n",
    "    #\"min_samples_split\":2\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "ext = ExtraTreesClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "ada = AdaBoostClassifier(**params2)\n",
    "grad = GradientBoostingClassifier(**params, max_features=np.int(np.sqrt(X.shape[1])))\n",
    "eclf = VotingClassifier(estimators=[('lr', logreg), ('gnb', gnb), ('rf', rf), ('ext', ext),('ada', ada), ('grad', grad)], voting='hard')\n",
    "\n",
    "for clf, label in zip([logreg, gnb, rf, ext,ada,grad,eclf], [\n",
    "    'Logistic Regression', 'naive Bayes',\n",
    "    'Random Forest', 'Extra Trees', \"AdaBoost\", \"Gradient Boosting\",'Voting Classfier']):\n",
    "    clf.fit(X,y)\n",
    "    cross_scores = cross_val_score(clf, X, y, scoring='accuracy', cv=3)\n",
    "    test_scores = accuracy_score(clf.predict(X_test), y_test)\n",
    "    print(\"Cross validation: %0.4f (+/- %0.2f) [%s]\" % (cross_scores.mean(), cross_scores.std(), label))\n",
    "    print(\"Test accuracy: %0.4f (+/- %0.2f) [%s]\" % (test_scores.mean(), test_scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = \"titanic\"\n",
    "dataset = \"spam\"\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 200\n",
    "\n",
    "if dataset == \"titanic\":\n",
    "    # Load titanic data\n",
    "    path_train = 'titanic_training.csv'\n",
    "    data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "    path_test = 'titanic_testing_data.csv'\n",
    "    test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
    "    y = data[1:, 0]  # label = survived\n",
    "    path_y_test = 'titanic_testing_labels_only.txt'\n",
    "    y_test = genfromtxt(path_y_test, delimiter=',', dtype=None)\n",
    "    class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "    labeled_idx = np.where(y != b'')[0]\n",
    "    y = np.array(y[labeled_idx], dtype=np.int)\n",
    "    print(\"Part (a): preprocessing the titanic dataset\")\n",
    "    X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
    "    X = X[labeled_idx, :]\n",
    "    Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "    assert X.shape[1] == Z.shape[1]\n",
    "    features = list(data[0, 1:]) + onehot_features\n",
    "\n",
    "elif dataset == \"spam\":\n",
    "    features = [\n",
    "        \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "        \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "        \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "        \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "        \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "        \"square_bracket\", \"ampersand\"\n",
    "    ]\n",
    "    assert len(features) == 32\n",
    "\n",
    "    # Load spam data\n",
    "    path_train = 'spam_data.mat'\n",
    "    data = scipy.io.loadmat(path_train)\n",
    "    X = data['training_data']\n",
    "    y = np.squeeze(data['training_labels'])\n",
    "    Z = data['test_data']\n",
    "    path_y_test = 'spam_test_labels_only.txt'\n",
    "    y_test = genfromtxt(path_y_test, delimiter=',', dtype=None)\n",
    "    class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "print('==============================================')\n",
    "print(\"Features\", features)\n",
    "print('==============================================')\n",
    "print(\"Train/test size\", X.shape, Z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b). Implement basic decision tree\n",
    "\n",
    "Implement the\n",
    "\n",
    "**information gain**, i.e., entropy of the parent node minus the weighted sum of entropy of the child nodes\n",
    "\n",
    "**Gini purification**, i.e., Gini impurity of the parent node minus the weighted sum of Gini impurities of the child nodes splitting rules for greedy decision tree learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        if y.size == 0:\n",
    "            return 0\n",
    "        p0 = np.where(y < 0.5)[0].size / y.size\n",
    "        if np.abs(p0) < 1e-10 or np.abs(1 - p0) < 1e-10:\n",
    "            return 0\n",
    "        # TODO: compute entropy_value\n",
    "        ### start entropy_code ###\n",
    "        entropy_value = -p0*np.log(p0) \n",
    "        ### end entropy_code ###\n",
    "\n",
    "        return entropy_value\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        base = DecisionTree.entropy(y)\n",
    "        y0 = y[np.where(X < thresh)[0]]\n",
    "        p0 = y0.size / y.size\n",
    "        y1 = y[np.where(X >= thresh)[0]]\n",
    "        p1 = y1.size / y.size\n",
    "        # TODO: compute entropy_children\n",
    "\n",
    "        ### start information_gain_code ###\n",
    "        entropy_children = p0*DecisionTree.entropy(y0) + p1*DecisionTree.entropy(y1)\n",
    "        ### end information_gain_code ###\n",
    "\n",
    "        return base - entropy_children\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        if y.size == 0:\n",
    "            return 0\n",
    "        p0 = np.where(y < 0.5)[0].size / y.size\n",
    "        if np.abs(p0) < 1e-10 or np.abs(1 - p0) < 1e-10:\n",
    "            return 0\n",
    "        # TODO: compute entropy_value\n",
    "\n",
    "        ### start gini_impurity_code ###\n",
    "        gini_impurity_value = p0*(1-p0) \n",
    "        ### end gini_impurity_code ###\n",
    "\n",
    "        return gini_impurity_value\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_purification(X, y, thresh):\n",
    "        base = DecisionTree.gini_impurity(y)\n",
    "        y0 = y[np.where(X < thresh)[0]]\n",
    "        p0 = y0.size / y.size\n",
    "        y1 = y[np.where(X >= thresh)[0]]\n",
    "        p1 = y1.size / y.size\n",
    "        # TODO: compute entropy_children\n",
    "\n",
    "        ### start gini_purification_code ###\n",
    "        gini_impurity_children = p0*DecisionTree.gini_impurity(y0) + p1*DecisionTree.gini_impurity(y1)\n",
    "        ### end gini_purification_code ###\n",
    "\n",
    "        return base - gini_impurity_children\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0:\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node\n",
    "            # splits.\n",
    "            thresh = np.array([\n",
    "                np.linspace(\n",
    "                    np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=10)\n",
    "                for i in range(X.shape[1])\n",
    "            ])\n",
    "            for i in range(X.shape[1]):\n",
    "                gains.append([\n",
    "                    self.information_gain(X[:, i], y, t) for t in thresh[i, :]\n",
    "                ])\n",
    "\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(\n",
    "                np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(\n",
    "                X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(\n",
    "                X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug = DecisionTree\n",
    "# #arr = np.random.rand(10,)\n",
    "# arr = np.array([0.1,1,1,1,1,1])\n",
    "# print(arr)\n",
    "# debug.entropy(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c). Train a shallow decision tree on the Titanic and Spam dataset and visualize your tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate the decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic decision tree\n",
    "max_depth = 3\n",
    "print('==============================================')\n",
    "print(\"Part (a, c): simplified decision tree\")\n",
    "dt = DecisionTree(max_depth=max_depth, feature_labels=features)\n",
    "dt.fit(X, y)\n",
    "print('==============================================')\n",
    "print(\"Tree structure\", dt.__repr__())\n",
    "\n",
    "# Basic decision tree\n",
    "print('==============================================')\n",
    "print(\"Part (c): sklearn's decision tree\")\n",
    "params = {\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "clf = DecisionTreeClassifier(random_state=0, **params)\n",
    "clf.fit(X, y)\n",
    "evaluate(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize your tree**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydot import graph_from_dot_data\n",
    "import io\n",
    "out = io.StringIO()\n",
    "export_graphviz(clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "# For OSX, may need the following for dot: brew install gprof2dot\n",
    "graph = graph_from_dot_data(out.getvalue())\n",
    "graph_from_dot_data(out.getvalue())[0].write_pdf(\"{}-tree-depth{}.pdf\".format(dataset, max_depth))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d). Implement bagged trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(random_state=i, **self.params) for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n):\n",
    "            idx = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "            newX, newy = X[idx, :], y[idx]\n",
    "            self.decision_trees[i].fit(newX, newy)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        yhat = [self.decision_trees[i].predict(X) for i in range(self.n)]\n",
    "        # TODO: compute yhat_avg for BaggedTrees\n",
    "        ### start code ###\n",
    "        yhat_avg = np.round(np.mean(yhat, axis=0), decimals=0)\n",
    "        ### end code ###\n",
    "        return yhat_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e). Apply bagged trees to the titanic and spam datasets.\n",
    "\n",
    "Find and state the most common splits made at the root node of the trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagged trees\n",
    "print(\"Part (e): bagged trees\")\n",
    "bt = BaggedTrees(params, n=N)\n",
    "bt.fit(X, y)\n",
    "evaluate(bt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f). Implement random forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(BaggedTrees):\n",
    "\n",
    "    def __init__(self, params=None, n=200, m=1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params['max_features'] = m\n",
    "        super().__init__(params=params, n=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (g). Apply bagged random forests to the titanic and spam datasets.\n",
    "\n",
    "Find and state the most common splits made at the root node of the trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "print(\"Part (g): random forest\")\n",
    "rf = RandomForest(params, n=N, m=np.int(np.sqrt(X.shape[1])))\n",
    "rf.fit(X, y)\n",
    "evaluate(rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (h). Implement AdaBoost algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostedRandomForest(RandomForest):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data\n",
    "        self.a = np.zeros(self.n)  # Weights on decision trees\n",
    "        i = 0\n",
    "        while i < self.n:\n",
    "            idx = np.random.choice(X.shape[0], size=X.shape[0], p=self.w)\n",
    "            newX, newy = X[idx, :], y[idx]\n",
    "            self.decision_trees[i].fit(newX, newy)\n",
    "            wrong = np.abs((y - self.decision_trees[i].predict(X)))\n",
    "            error = wrong.dot(self.w) / np.sum(self.w)\n",
    "            self.a[i] = 0.5 * np.log((1 - error) / error)\n",
    "            # Update w\n",
    "            wrong_idx = np.where(wrong > 0.5)[0]\n",
    "            right_idx = np.where(wrong <= 0.5)[0]\n",
    "            # TODO: fill in the code for updating 'self.w'\n",
    "            \n",
    "            ### start code ###\n",
    "            self.w[wrong_idx] = self.w[wrong_idx]*np.exp(self.a[i])\n",
    "            self.w[right_idx] = self.w[right_idx]*np.exp(-self.a[i])\n",
    "            self.w = self.w / sum(self.w)\n",
    "            #print(self.w)\n",
    "            i += 1\n",
    "        ### end code ###\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        yhat = [self.a[i]*self.decision_trees[i].predict(X) for i in range(self.n)]\n",
    "        yhat_BoostedRandomForest = np.round(np.sum(yhat, axis=0) / np.sum(self.a))\n",
    "        return yhat_BoostedRandomForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted random forest\n",
    "print(\"Part (h): boosted random forest\")\n",
    "boosted = BoostedRandomForest(params, n=N, m=np.int(np.sqrt(X.shape[1])))\n",
    "boosted.fit(X, y)\n",
    "evaluate(boosted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (i). Summarize the performance evaluation of: a single decision tree, bagged trees, random forests, and boosted trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Part (0): constant classifier\n",
      "constant classifier test accuracy:  0.4531330916623511\n",
      "==============================================\n",
      "Part (c): simplified decision tree\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-4884b437c212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=============================================='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Part (c): simplified decision tree\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Basic decision tree test accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTree' is not defined"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 200\n",
    "max_depth = 5\n",
    "\n",
    "# Constant classifier\n",
    "print('==============================================')\n",
    "print(\"Part (0): constant classifier\")\n",
    "print('constant classifier test accuracy: ', accuracy_score(np.zeros_like(y_test), y_test))\n",
    "\n",
    "# Basic decision tree\n",
    "print('==============================================')\n",
    "print(\"Part (c): simplified decision tree\")\n",
    "dt = DecisionTree(max_depth=max_depth, feature_labels=features)\n",
    "dt.fit(X, y)\n",
    "print('Basic decision tree test accuracy: ', accuracy_score(dt.predict(Z), y_test))\n",
    "\n",
    "# Basic decision tree\n",
    "print('==============================================')\n",
    "print(\"Part (c): sklearn's decision tree\")\n",
    "params = {\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "clf = DecisionTreeClassifier(random_state=0, **params)\n",
    "clf.fit(X, y)\n",
    "evaluate(clf, print_splits=True)\n",
    "print('sklearn decision tree test accuracy: ', accuracy_score(clf.predict(Z), y_test))\n",
    "\n",
    "\n",
    "# Bagged trees\n",
    "print('==============================================')\n",
    "print(\"Part (e): bagged trees\")\n",
    "bt = BaggedTrees(params, n=N)\n",
    "bt.fit(X, y)\n",
    "evaluate(bt, print_splits=True)\n",
    "print('bagged trees test accuracy: ', accuracy_score(bt.predict(Z), y_test))\n",
    "\n",
    "\n",
    "# Random forest\n",
    "print('==============================================')\n",
    "print(\"Part (g): random forest\")\n",
    "rf = RandomForest(params, n=N, m=np.int(np.sqrt(X.shape[1])))\n",
    "rf.fit(X, y)\n",
    "evaluate(rf, print_splits=True)\n",
    "print('random forest test accuracy: ', accuracy_score(rf.predict(Z), y_test))\n",
    "\n",
    "\n",
    "# Boosted random forest\n",
    "print('==============================================')\n",
    "print(\"Part (h): boosted random forest\")\n",
    "boosted = BoostedRandomForest(params, n=N, m=np.int(np.sqrt(X.shape[1])))\n",
    "boosted.fit(X, y)\n",
    "evaluate(boosted, print_splits=True)\n",
    "print('boosted random forest test accuracy: ', accuracy_score(boosted.predict(Z), y_test))\n",
    "print(dataset)\n",
    "np.savetxt('{}.out'.format(dataset), boosted.predict(Z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (j). For the spam dataset only: Describe what kind of data are the most challenging to classify and which are the easiest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'titanic':\n",
    "    import sys\n",
    "    sys.exit()\n",
    "# The following is for the spam dataset only.\n",
    "# Sample code for determining which data are easier/harder to classify\n",
    "print(\"\\n\\nPart (i): easy/hard examples\")\n",
    "A = np.argsort(boosted.w)\n",
    "tail = 1000\n",
    "bits = np.array([np.sum(X[A[i], :-7]) for i in range(X.shape[0])])\n",
    "print(\n",
    "    \"Number of easiest %s samples containing >= 2 feature counts:\" % tail,\n",
    "    np.sum(bits[:tail] >= 2))\n",
    "print(\"Number of hardest %s samples containing < 2 feature counts:\" % tail,\n",
    "      np.sum(bits[-tail:] < 2))\n",
    "bits = np.array([np.sum(X[A[i], :]) for i in range(X.shape[0])])\n",
    "print(\n",
    "    \"Number of easiest %s samples containing >= 5 feature counts:\" % tail,\n",
    "    np.sum(bits[:tail] >= 5))\n",
    "print(\"Number of hardest %s samples containing < 5 feature counts:\" % tail,\n",
    "      np.sum(bits[-tail:] < 5))\n",
    "idxes = 200 - np.where(bits[-200:] > 7)[0]\n",
    "print(\"Most challenging samples: Examples among the 200 data most likely \"\n",
    "      \"to be sampled which contained at least 7 feature counts\")\n",
    "for idx in idxes:\n",
    "    print('Example:', class_names[y[A[-idx]]],\n",
    "          [x for x in zip(features, X[A[-idx], :]) if x[1] > 0],\n",
    "          bits[-idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
